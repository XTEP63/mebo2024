{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2. Inferencia básica y máxima verosimilitud\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://storage.needpix.com/rsynced_images/bayesian-2889576_1280.png\" width=\"200px\" height=\"180px\" />\n",
    "\n",
    "En esta segunda tarea, repasarás algunos ejercicios de inferencia en modelos probabilísticos básicos. Además, tendrás la oportunidad de mostrar como estimar los parámetros de una distribución normal. Finalmente explorarás cómo es el producto de dos densidades normales.\n",
    "\n",
    "Por favor, intenta ser lo más explícit@ posible, y en lo posible, apóyate de la escritura matemática con $\\LaTeX$.\n",
    "\n",
    "Recuerda además que ante cualquier duda, me puedes contactar al correo esjimenezro@iteso.mx.\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://storage.needpix.com/rsynced_images/bayesian-2889576_1280.png.</p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "Consideramos un conjunto de datos \n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{x_1, \\dots, x_N\\}\n",
    "$$\n",
    "\n",
    "independientes e idénticamente distribuidos, los cuales suponemos que provienen de una distribución normal $\\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "\n",
    "Muestre que los estimadores de máxima verosimilitud para los parámetros $\\mu$ y $\\sigma^2$ son:\n",
    "\n",
    "$$\\hat{\\mu}_{MLE} = \\frac{1}{N} \\sum_{j=1}^{N}x_j \\qquad \\text{y} \\qquad \\hat{\\sigma}_{MLE}^2 = \\frac{1}{N}\\sum_{j=1}^{N}(x_j-\\hat{\\mu}_{MLE})^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de densidad de probabilidad para un dato \\( x_j \\) es:\n",
    "\n",
    "$$\n",
    "f(x_j | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu)^2}{2\\sigma^2}\\right).\n",
    "$$\n",
    "\n",
    "Dado que los datos son independientes e idénticamente distribuidos (i.i.d.), la función de verosimilitud conjunta es:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2 | x_1, \\dots, x_N) = \\prod_{j=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu)^2}{2\\sigma^2}\\right).\n",
    "$$\n",
    "\n",
    "Tomando el logaritmo de la función de verosimilitud, obtenemos la log-verosimilitud:\n",
    "\n",
    "$$\n",
    "\\log L(\\mu, \\sigma^2 | x_1, \\dots, x_N) = -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{N} (x_j - \\mu)^2.\n",
    "$$\n",
    "\n",
    "### Paso 1: Maximización con respecto a \\( \\mu \\)\n",
    "\n",
    "Tomamos la derivada de la log-verosimilitud con respecto a \\( \\mu \\) y la igualamos a cero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} \\log L(\\mu, \\sigma^2 | x_1, \\dots, x_N) = \\frac{1}{\\sigma^2} \\sum_{j=1}^{N} (x_j - \\mu) = 0.\n",
    "$$\n",
    "\n",
    "De aquí, resolvemos para \\( \\mu \\):\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{N} (x_j - \\mu) = 0 \\quad \\Rightarrow \\quad \\mu = \\frac{1}{N} \\sum_{j=1}^{N} x_j.\n",
    "$$\n",
    "\n",
    "Por lo tanto, el estimador de máxima verosimilitud para \\( \\mu \\) es:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{MLE} = \\frac{1}{N} \\sum_{j=1}^{N} x_j.\n",
    "$$\n",
    "\n",
    "### Paso 2: Maximización con respecto a \\( \\sigma^2 \\)\n",
    "\n",
    "Tomamos la derivada de la log-verosimilitud con respecto a \\( \\sigma^2 \\) y la igualamos a cero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\sigma^2} \\log L(\\mu, \\sigma^2 | x_1, \\dots, x_N) = -\\frac{N}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{j=1}^{N} (x_j - \\mu)^2 = 0.\n",
    "$$\n",
    "\n",
    "Multiplicamos por \\( \\sigma^4 \\) para simplificar y resolvemos para \\( \\sigma^2 \\):\n",
    "\n",
    "$$\n",
    "-\\frac{N}{2} \\sigma^2 + \\frac{1}{2} \\sum_{j=1}^{N} (x_j - \\mu)^2 = 0 \\quad \\Rightarrow \\quad \\sigma^2 = \\frac{1}{N} \\sum_{j=1}^{N} (x_j - \\mu)^2.\n",
    "$$\n",
    "\n",
    "Sustituyendo \\( \\hat{\\mu}_{MLE} \\) en la fórmula, obtenemos:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2_{MLE} = \\frac{1}{N} \\sum_{j=1}^{N} (x_j - \\hat{\\mu}_{MLE})^2.\n",
    "$$\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Los estimadores de máxima verosimilitud para \\( \\mu \\) y \\( \\sigma^2 \\) son:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{MLE} = \\frac{1}{N} \\sum_{j=1}^{N} x_j \\quad \\text{y} \\quad \\hat{\\sigma}^2_{MLE} = \\frac{1}{N} \\sum_{j=1}^{N} (x_j - \\hat{\\mu}_{MLE})^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "Consideramos un conjunto de datos \n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{x_1, \\dots, x_N\\}\n",
    "$$\n",
    "\n",
    "independientes e idénticamente distribuidos, los cuales suponemos que provienen de una distribución Gamma, con la siguiente pdf\n",
    "\n",
    "$$\n",
    "p(x | a) = \\frac{a^5}{4!} x^4 e^{-ax}\n",
    "$$\n",
    "\n",
    "Encuentre el estimador de máxima verosimilitud para $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1: Función de verosimilitud\n",
    "\n",
    "Dado que los datos D son independientes e idénticamente distribuidos (i.i.d.), la función de verosimilitud conjunta es el producto de las funciones de densidad para cada dato \\( x_j \\):\n",
    "\n",
    "$$\n",
    "L(a | x_1, \\dots, x_N) = \\prod_{j=1}^{N} \\frac{a^5}{4!} x_j^4 e^{-a x_j}.\n",
    "$$\n",
    "\n",
    "Podemos simplificar la verosimilitud como:\n",
    "\n",
    "$$\n",
    "L(a | x_1, \\dots, x_N) = \\left( \\frac{a^5}{4!} \\right)^N \\prod_{j=1}^{N} x_j^4 e^{-a x_j}.\n",
    "$$\n",
    "\n",
    "### Paso 2: Log-verosimilitud\n",
    "\n",
    "Para simplificar los cálculos, tomamos el logaritmo natural de la función de verosimilitud (log-verosimilitud):\n",
    "\n",
    "$$\n",
    "\\log L(a | x_1, \\dots, x_N) = N \\log\\left( \\frac{a^5}{4!} \\right) + \\sum_{j=1}^{N} \\log(x_j^4) - a \\sum_{j=1}^{N} x_j.\n",
    "$$\n",
    "\n",
    "Podemos descomponer esta expresión en términos más simples:\n",
    "\n",
    "$$\n",
    "\\log L(a | x_1, \\dots, x_N) = N \\log\\left( \\frac{a^5}{4!} \\right) + 4 \\sum_{j=1}^{N} \\log(x_j) - a \\sum_{j=1}^{N} x_j.\n",
    "$$\n",
    "\n",
    "Usando la propiedad de los logaritmos, \\( \\log\\left( \\frac{a^5}{4!} \\right) = 5 \\log(a) - \\log(4!) \\), obtenemos:\n",
    "\n",
    "$$\n",
    "\\log L(a | x_1, \\dots, x_N) = N [5 \\log(a) - \\log(24)] + 4 \\sum_{j=1}^{N} \\log(x_j) - a \\sum_{j=1}^{N} x_j.\n",
    "$$\n",
    "\n",
    "### Paso 3: Maximización con respecto a \\( a \\)\n",
    "\n",
    "Para encontrar el estimador de máxima verosimilitud para \\( a \\), tomamos la derivada de la log-verosimilitud con respecto a \\( a \\) y la igualamos a cero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial a} \\log L(a | x_1, \\dots, x_N) = \\frac{5N}{a} - \\sum_{j=1}^{N} x_j = 0.\n",
    "$$\n",
    "\n",
    "Resolviendo para \\( a \\), obtenemos:\n",
    "\n",
    "$$\n",
    "a = \\frac{5N}{\\sum_{j=1}^{N} x_j}.\n",
    "$$\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "El estimador de máxima verosimilitud (MLE) para \\( a \\) es:\n",
    "\n",
    "$$\n",
    "\\hat{a}_{MLE} = \\frac{5N}{\\sum_{j=1}^{N} x_j}.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
